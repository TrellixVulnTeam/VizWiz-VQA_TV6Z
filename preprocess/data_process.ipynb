{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取JSON数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from stop_list_0 import closed_class_stop_words as words\n",
    "from collections import Counter\n",
    "from OCR_utils import getocr, question_processing, alignScore, editDist, alignScore1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = json.load(open('train.json', 'r',encoding='UTF-8'))\n",
    "unans = ['unanswerable', 'unsuitable', 'unsuitable image', 'image unsuitable', 'unsure', 'no unsuitable image', 'unsuitable imagine', 'unauunsuit image', 'unknown', 'unknow']\n",
    "for item in val_data:\n",
    "    for ans in item['answers']:\n",
    "        if ans['answer'] in unans:\n",
    "            ans['answer'] = 'unanswerable'\n",
    "with open('train.json', 'w') as f:\n",
    "    json.dump(val_data, f, indent = 4, separators = (',', ':'))\n",
    "val_data = json.load(open('val.json', 'r',encoding='UTF-8'))\n",
    "unans = ['unanswerable', 'unsuitable', 'unsuitable image', 'image unsuitable', 'unsure', 'no unsuitable image', 'unsuitable imagine', 'unauunsuit image', 'unknown', 'unknow']\n",
    "for item in val_data:\n",
    "    for ans in item['answers']:\n",
    "        if ans['answer'] in unans:\n",
    "            ans['answer'] = 'unanswerable'\n",
    "with open('val.json', 'w') as f:\n",
    "    json.dump(val_data, f, indent = 4, separators = (',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path1->目标原json文件, path2->目标OCR_RESULT\n",
    "ocrdic, val_data = getocr('train.json', 'train_ocr_result.json')\n",
    "with open('train_newocr.json', 'w') as f:\n",
    "    json.dump(ocrdic, f, indent = 4, separators = (',', ':'))\n",
    "with open('train_newtag.json', 'w') as f:\n",
    "    json.dump(val_data, f, indent = 4, separators = (',', ':'))\n",
    "\n",
    "#path1->目标原json文件, path2->目标OCR_RESULT\n",
    "ocrdic, val_data = getocr('val.json', 'val_ocr_result.json')\n",
    "with open('val_newocr.json', 'w') as f:\n",
    "    json.dump(ocrdic, f, indent = 4, separators = (',', ':'))\n",
    "with open('val_newtag.json', 'w') as f:\n",
    "    json.dump(val_data, f, indent = 4, separators = (',', ':'))\n",
    "\n",
    "# #path1->目标原json文件, path2->目标OCR_RESULT\n",
    "# ocrdic, val_data = getocr('train_val.json', 'train_val_ocr_result.json')\n",
    "# with open('train_val_newocr.json', 'w') as f:\n",
    "#     json.dump(ocrdic, f, indent = 4, separators = (',', ':'))\n",
    "# with open('train_val_newtag.json', 'w') as f:\n",
    "#     json.dump(val_data, f, indent = 4, separators = (',', ':'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current # of words in label list: 3000\n"
     ]
    }
   ],
   "source": [
    "#提取停用词\n",
    "stoplist = words\n",
    "# print(stoplist)\n",
    "keylist = [] #所有问题的关键词列表，未去重\n",
    "total_Tsent = [] #所有问题的包含关键词的句子列表，去重\n",
    "val_data = json.load(open('train_newtag.json', 'r',encoding='UTF-8'))\n",
    "for ques in val_data:\n",
    "    #去除answer_type = \"unanssent = []\n",
    "    sent = set()\n",
    "    for i in range(10):\n",
    "        sent.add(ques['answers'][i]['answer'])\n",
    "    num = Counter(sent)\n",
    "    # #选出关键词和包含关键词的句子\n",
    "    #答案的关键词和包含关键词的句子加入total_Tsent\n",
    "    total_Tsent.extend(list(sent))\n",
    "keydict = Counter(total_Tsent) #统计关键词频\n",
    "# Sort the keydict, and then find its first 3000 words;\n",
    "keydict = sorted(keydict.items(), key = lambda x:x[1], reverse=True)\n",
    "# onelis = []\n",
    "# for it in keydict:\n",
    "#     if it[1] == 1:\n",
    "#         onelis.append(it[0])\n",
    "# count = 0\n",
    "# key3000 = []\n",
    "# for it in keydict:\n",
    "#     count += 1\n",
    "#     if count <= 3000:\n",
    "#         key3000.append(it[0])\n",
    "#     else:\n",
    "#         break\n",
    "# imgcount = 0\n",
    "# for ques in val_data:\n",
    "#     for ans in ques['answers']:\n",
    "#         if ans['answer'] in key3000:\n",
    "#             imgcount += 1\n",
    "#             break\n",
    "# print('We select ',len(key3000), ' keys')\n",
    "# print('Total image number: ', len(val_data))\n",
    "# print('Lables cover ', imgcount, ' images')\n",
    "label = []\n",
    "count = 0\n",
    "for item in keydict:\n",
    "    if count == 3000:\n",
    "        break\n",
    "    label.append(item[0])\n",
    "    count += 1\n",
    "print('Current # of words in label list: %d' %(len(label)))\n",
    "###\n",
    "###\n",
    "### Dump label and its index mapping into JSON files;\n",
    "###\n",
    "###\n",
    "label_dict = {}\n",
    "for i in range(len(label)):\n",
    "    label_dict[label[i]] = i\n",
    "with open('labelmap.json', 'w') as f:\n",
    "    json.dump(label_dict, f, indent = 4, separators = (',', ':'))\n",
    "with open('label.json', 'w') as f:\n",
    "    json.dump(label, f, indent = 4, separators = (',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imagelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train answers num is:  14991\n",
      "Remain train answers num is:  14130\n",
      "Train answers max is ocr num is:  731\n",
      "Total val answers num is:  2934\n",
      "Remain val answers num is:  2713\n",
      "Train answers max is ocr num is:  169\n"
     ]
    }
   ],
   "source": [
    "qid = 0\n",
    "val_data = json.load(open('train_newtag.json','r',encoding='UTF-8'))\n",
    "answer = []\n",
    "print('Total train answers num is: ', len(val_data))\n",
    "ocrcount= 0\n",
    "for ques in val_data:\n",
    "    ansdict = {}\n",
    "    ansdict[\"answer_type\"] = ques[\"answer_type\"]\n",
    "    ansdict['img_id'] = ques['image'].replace('.jpg','')\n",
    "    ansdict['label'] = {}\n",
    "    ansdict['question_id'] = qid\n",
    "    qid += 1\n",
    "    ansdict['sent'] = question_processing(ques['question'])\n",
    "    sent = []\n",
    "    for i in range(10):\n",
    "        sent.append(ques['answers'][i]['answer'])\n",
    "    num = Counter(sent)\n",
    "    dictionary = sorted(num.items(), key = lambda x:x[1], reverse=True)\n",
    "    ###\n",
    "    ###\n",
    "    ### Soft annotation here based on word freqiency;\n",
    "    ###\n",
    "    ###\n",
    "    ans_count = 0\n",
    "    if dictionary[0][0] in [\"OCR1\", \"OCR2\", \"OCR3\"]:\n",
    "        ocrcount += 1\n",
    "    for ans in dictionary:\n",
    "        if ans[0] in label:\n",
    "            ans_count += ans[1]\n",
    "    if ans_count == 0:\n",
    "        continue\n",
    "    else:\n",
    "        for ans in dictionary:\n",
    "            if ans[0] in label:\n",
    "                ansdict['label'][ans[0]] = float(ans[1] / ans_count)\n",
    "    answer.append(ansdict)\n",
    "print('Remain train answers num is: ', len(answer))\n",
    "print('Train answers max is ocr num is: ', ocrcount)\n",
    "with open('train_imglabel.json', 'w') as f:\n",
    "    json.dump(answer, f, indent = 4, separators = (',', ':'))\n",
    "\n",
    "qid = 0\n",
    "val_data = json.load(open('val_newtag.json','r',encoding='UTF-8'))\n",
    "newanswer = []\n",
    "# answer = []\n",
    "print('Total val answers num is: ', len(val_data))\n",
    "vallis = []\n",
    "ocrcount = 0\n",
    "for ques in val_data:\n",
    "    ansdict = {}\n",
    "    ansdict[\"answer_type\"] = ques[\"answer_type\"]\n",
    "    ansdict['img_id'] = ques['image'].replace('.jpg','')\n",
    "    ansdict['label'] = {}\n",
    "    ansdict['question_id'] = qid\n",
    "    qid += 1\n",
    "    ansdict['sent'] = question_processing(ques['question'])\n",
    "    sent = []\n",
    "    for i in range(10):\n",
    "        sent.append(ques['answers'][i]['answer'])\n",
    "    num = Counter(sent)\n",
    "    dictionary = sorted(num.items(), key = lambda x:x[1], reverse=True)\n",
    "    ###\n",
    "    ###\n",
    "    ### Soft annotation here based on word freqiency;\n",
    "    ###\n",
    "    ###\n",
    "    ans_count = 0\n",
    "    if dictionary[0][0] in [\"OCR1\", \"OCR2\", \"OCR3\"]:\n",
    "        ocrcount += 1\n",
    "    for ans in dictionary:\n",
    "        if ans[0] in label:\n",
    "            ans_count += ans[1]\n",
    "    if ans_count == 0:\n",
    "        vallis.append(ques)\n",
    "    else:\n",
    "        for ans in dictionary:\n",
    "            if ans[0] in label:\n",
    "                ansdict['label'][ans[0]] = float(ans[1] / ans_count)\n",
    "        answer.append(ansdict)\n",
    "        newanswer.append(ansdict)\n",
    "print('Remain val answers num is: ', len(newanswer))\n",
    "print('Train answers max is ocr num is: ', ocrcount)\n",
    "with open('val_imglabel.json', 'w') as f:\n",
    "    json.dump(newanswer, f, indent = 4, separators = (',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR-with-ANS OCR-without-ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getocrwithans(path):\n",
    "    val_data = json.load(open(path,'r',encoding='UTF-8'))\n",
    "    for item in val_data:\n",
    "        if val_data[item] == []:\n",
    "            continue\n",
    "        else:\n",
    "            for data in val_data[item]:\n",
    "                del data[1]\n",
    "    return val_data\n",
    "def getocrwithoutans(path):\n",
    "    val_data = json.load(open(path,'r',encoding='UTF-8'))\n",
    "    for item in val_data:\n",
    "        if val_data[item] == []:\n",
    "            continue\n",
    "        else:\n",
    "            for data in val_data[item]:\n",
    "                del data[2]\n",
    "    return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_with_ans = getocrwithans('train_newocr.json')\n",
    "ocr_without_ans = getocrwithoutans('train_newocr.json')\n",
    "with open('train_ocr_with_ans.json', 'w') as f:\n",
    "    json.dump(ocr_with_ans, f, indent = 4, separators = (',', ':'))\n",
    "with open('train_ocr_without_ans.json', 'w') as f:\n",
    "    json.dump(ocr_without_ans, f, indent = 4, separators = (',', ':'))\n",
    "ocr_with_ans = getocrwithans('val_newocr.json')\n",
    "ocr_without_ans = getocrwithoutans('val_newocr.json')\n",
    "with open('val_ocr_with_ans.json', 'w') as f:\n",
    "    json.dump(ocr_with_ans, f, indent = 4, separators = (',', ':'))\n",
    "with open('val_ocr_without_ans.json', 'w') as f:\n",
    "    json.dump(ocr_without_ans, f, indent = 4, separators = (',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选掉OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(label))\n",
    "for it in label:\n",
    "    if it in ['OCR1', 'OCR2', 'OCR3']:\n",
    "        label.remove(it)\n",
    "label_dict = {}\n",
    "for i in range(len(label)):\n",
    "    label_dict[label[i]] = i\n",
    "with open('labelmap_without_OCR.json', 'w') as f:\n",
    "    json.dump(label_dict, f, indent = 4, separators = (',', ':'))\n",
    "with open('label_without_OCR.json', 'w') as f:\n",
    "    json.dump(label, f, indent = 4, separators = (',', ':'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = json.load(open('train_imglabel.json','r',encoding='UTF-8'))\n",
    "ocrlis = ['OCR1', 'OCR2', 'OCR3']\n",
    "val_data_noocr = []\n",
    "for item in val_data:    \n",
    "    middic = {}\n",
    "    for ans in item['label'].keys():\n",
    "        if ans not in ocrlis:\n",
    "            middic[ans] = item['label'][ans]\n",
    "    if len(middic) != 0:\n",
    "        item['label'] = middic\n",
    "        val_data_noocr.append(item)\n",
    "with open('train_imagelabel_noocr.json', 'w') as f:\n",
    "    json.dump(val_data_noocr, f, indent = 4, separators = (',', ':'))\n",
    "val_data = json.load(open('val_imglabel.json','r',encoding='UTF-8'))\n",
    "ocrlis = ['OCR1', 'OCR2', 'OCR3']\n",
    "val_data_noocr = []\n",
    "for item in val_data:    \n",
    "    middic = {}\n",
    "    for ans in item['label'].keys():\n",
    "        if ans not in ocrlis:\n",
    "            middic[ans] = item['label'][ans]\n",
    "    if len(middic) != 0:\n",
    "        item['label'] = middic\n",
    "        val_data_noocr.append(item)\n",
    "with open('val_imagelabel_noocr.json', 'w') as f:\n",
    "    json.dump(val_data_noocr, f, indent = 4, separators = (',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdata = json.load(open('RES/val_predict.json'))\n",
    "ocrlis = ['OCR1', 'OCR2', 'OCR3']\n",
    "ocrdata = []\n",
    "for item in resdata:\n",
    "    if item['answer'] in ocrlis and item[\"answer_type\"] != \"unanswerable\":\n",
    "        ocrdata.append(item)\n",
    "with open('RES/val_predict.json', 'w') as f:\n",
    "    json.dump(ocrdata, f, indent = 4, separators = (',', ':'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8164e307c92428f22d56d2c9a067160dda965c96d06d8a62ec7acd5049e5c322"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
